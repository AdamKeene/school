{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Names:\n",
        "1. Jared Acord    (SID: 28446559)\n",
        "2. Adam Keene     (SID: 26197478)\n",
        "3. Brandon Lau    (SID: 18946278)\n",
        "4. Jackson Bolcer (SID: 22144453)"
      ],
      "metadata": {
        "id": "WC8ZcbP-usn0"
      },
      "id": "WC8ZcbP-usn0"
    },
    {
      "cell_type": "markdown",
      "id": "4d26ad9e",
      "metadata": {
        "id": "4d26ad9e"
      },
      "source": [
        "## Assignment 2: Code Whiteboard Tutor\n",
        "\n",
        "Main goal: Use a multimodal Large Language Model (LLM) to build a UI application that allows users to upload a photo of their handwritten Python code and receive suggestions for code improvements.\n",
        "\n",
        "The main functionalities that must be included are:\n",
        "- Transcribing handwritten Python code into a code snippet in\n",
        "- Running static analysis and explaining bugs (if any) in natural language\n",
        "- Suggesting bug fixes, improvements, or efficiency tweaks to the code snippet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27ad234",
      "metadata": {
        "id": "f27ad234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8c4cbd-56c4-4564-a251-0fb9df5960fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip -q install gradio unstructured sentence-transformers\n",
        "%pip -q install google.generativeai     # for using local IDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e945c6e5",
      "metadata": {
        "id": "e945c6e5"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "# from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ada105",
      "metadata": {
        "id": "48ada105"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "MODEL_ID = \"gemini-2.5-flash-lite\"\n",
        "genai.configure(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whAUgumrpOWH",
        "outputId": "f8b4277d-041c-4acb-f66e-9bf4794ee12d"
      },
      "id": "whAUgumrpOWH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path used in testing (not needed for app, just included for illustration)\n",
        "GOOGLE_DRIVE_DIR_PREFIX = \"drive/MyDrive/Classes/SWE270P/Assignments/SWE270P_A2/\"\n",
        "path_to_input_images = GOOGLE_DRIVE_DIR_PREFIX + \"code_images_example/\"\n",
        "path_to_output_images = GOOGLE_DRIVE_DIR_PREFIX + \"revised_code/\""
      ],
      "metadata": {
        "id": "FkpfxNr1nPL6"
      },
      "id": "FkpfxNr1nPL6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aKZLU4GmmmbW",
      "metadata": {
        "id": "aKZLU4GmmmbW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "485a58f6-78bd-4435-85a3-852c017cc233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Code transcription from image ######\n",
            "\n",
            "\n",
            "def bucketSort(arr, k):\n",
            "    counts = [0] * k\n",
            "    for x in arr:\n",
            "        counts[x] += 1\n",
            "\n",
            "    sorted_arr = []\n",
            "    for i, count in enumerate(arr):\n",
            "        sorted_arr.extend([i] * count)\n",
            "\n",
            "    return sorted_arr\n",
            "\n",
            "\n",
            "##### Code Analysis ######\n",
            "\n",
            "### Overview ###\n",
            "The provided `bucketSort` function implements a counting sort algorithm. It first counts the occurrences of each element within a given range `k`, and then reconstructs the sorted array based on these counts.\n",
            "\n",
            "### Bugs ###\n",
            "**Bug 1**: The second loop iterates over `arr` instead of `counts`. This will lead to an incorrect number of elements being added to `sorted_arr` and potentially an `IndexError` if elements in `arr` are greater than or equal to `k`.\n",
            "    **Fix**: Change `for i, count in enumerate(arr):` to `for i, count in enumerate(counts):` to iterate over the `counts` list.\n",
            "\n",
            "### Enhancements ###\n",
            "1.  **Input Validation**: The code assumes all elements in `arr` are non-negative and less than `k`. Adding checks for this would make the function more robust.\n",
            "2.  **Efficiency**: The current reconstruction method using `extend` is generally efficient. For extremely large counts, a list comprehension might offer a marginal improvement, but the primary bottleneck is the incorrect logic in the reconstruction loop.\n",
            "3.  **Clarity**: The variable names are clear. The bug in the second loop significantly impacts clarity.\n",
            "\n",
            "##### Revised Code ######\n",
            "\n",
            "\n",
            "def bucketSort(arr, k):\n",
            "    counts = [0] * k\n",
            "    for x in arr:\n",
            "        # Add input validation to ensure x is within the expected range [0, k-1]\n",
            "        if not (0 <= x < k):\n",
            "            raise ValueError(f\"Element {x} is out of the expected range [0, {k-1}]\")\n",
            "        counts[x] += 1\n",
            "\n",
            "    sorted_arr = []\n",
            "    # Iterate over the counts list to reconstruct the sorted array.\n",
            "    for i, count in enumerate(counts):\n",
            "        sorted_arr.extend([i] * count)\n",
            "\n",
            "    return sorted_arr\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "OTHER_PROGRAMMING_LANG = \"other\"\n",
        "\n",
        "# Define the gen config to be more conservative\n",
        "code_analysis_config = genai.types.GenerationConfig(temperature=0.1)\n",
        "\n",
        "\n",
        "def transcribe_image_code(image_file: Image.Image, code_language: str) -> str:\n",
        "  \"\"\"\n",
        "  Step 1: Transcribes code from the input image using a multimodal LLM.\n",
        "\n",
        "  Parameters:\n",
        "      image_file (Image.Image): The uploaded image containing handwritten code.\n",
        "      code_language (str): The programming language of the code.\n",
        "  Returns:\n",
        "      str: The transcribed code as a string.\n",
        "  \"\"\"\n",
        "  if code_language.lower().strip() == OTHER_PROGRAMMING_LANG:\n",
        "    prompt = \"Transcribe the handwritten code in this image. It can be of any programming language. Only return the code and nothing else. Only return the code within triple backticks, and do not include any language identifier after the opening triple backticks.\"\n",
        "  else:\n",
        "    prompt = f\"Transcribe the handwritten {code_language} code in this image. Only return the code and nothing else. Only return the code within triple backticks, and do not include any language identifier after the opening triple backticks.\"\n",
        "\n",
        "  prompt += f\"\"\"\n",
        "Here is an example of the output after transcribing the code in the image:\n",
        "```\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "```\n",
        "\n",
        "Notice how there is no language identifier after the opening triple backticks. This is an example of wrong output:\n",
        "```python\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "```\n",
        "\n",
        "Remember, no language identifier after the opening triple backticks after transcribing the code. I used Python in the above example, but it could be any programming language.\n",
        "\"\"\"\n",
        "\n",
        "  try:\n",
        "    # Initialize the multimodal model\n",
        "    model = genai.GenerativeModel(model_name=MODEL_ID)\n",
        "\n",
        "    # Create content with the image and a prompt\n",
        "    contents = [prompt, image_file]\n",
        "\n",
        "    # Generate content from the model\n",
        "    response = model.generate_content(contents)\n",
        "\n",
        "    # Get the response text, and filter out any decorators\n",
        "    ai_msg_content = response.text\n",
        "\n",
        "    ai_msg_content = ai_msg_content.replace(\"```python\", \"\")\n",
        "    ai_msg_content = ai_msg_content.replace(\"```java\", \"\")\n",
        "    ai_msg_content = ai_msg_content.replace(\"```\", \"\")\n",
        "\n",
        "    # Return the transcribed text\n",
        "    return ai_msg_content\n",
        "\n",
        "  except Exception as e:\n",
        "    raise gr.Error(f\"Error during image transcription: {str(e)}\")\n",
        "\n",
        "\n",
        "def analyze_code(code_block: str, code_language: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Step 2: Performs static analysis using LLM on the transcribed code.\n",
        "\n",
        "    Parameters:\n",
        "        code_block (str): The transcribed code snippet.\n",
        "        code_language (str): The programming language of the code.\n",
        "    Returns:\n",
        "        (1) The analysis text explaining bugs, improvements, or efficiency suggestions.\n",
        "        (2) The refined version of the code snippet with the suggested fixes and enhancements.\n",
        "    \"\"\"\n",
        "\n",
        "    if not code_block:\n",
        "        raise gr.Error(\"Please provide a valid code snippet.\")\n",
        "\n",
        "    if code_language.lower().strip() == OTHER_PROGRAMMING_LANG:\n",
        "      prompt = f\"\"\"\n",
        "You are an expert software developer and code reviewer. Analyze the following code.\n",
        "First, identify the programming language of the code. Then, provide a code review with suggestions for bugs,\n",
        "improvements, and efficiency tweaks based on the identified language.\n",
        "\"\"\"\n",
        "    else:\n",
        "      prompt = f\"You are an expert {code_language} developer and code reviewer. Analyze the following {code_language} code.\\n\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(MODEL_ID)\n",
        "\n",
        "        prompt = prompt + f\"\"\"\n",
        "The code probably contains one or more subtle or obvious bugs. Concisely give various\n",
        "improvements and efficiency tweaks to the code, and explain the solutions to any bugs found.\n",
        "When suggesting these changes, do not return the entire code, but rather reference only one\n",
        "or a few lines as needed. Keep each suggestion brief and summerized.\n",
        "\n",
        "After all suggested changes, return also a final, revised version of the code\n",
        "which incorporates all changes you see fit within a single code block. Be careful not to rename the method names.\n",
        "\n",
        "The format of your response must be as follows:\n",
        "\n",
        "### Overview ###\n",
        "(Give a brief overview of the code)\n",
        "### Bugs ###\n",
        "(A numbered list any bugs found, and their proposed resolution)\n",
        "### Enhancements ###\n",
        "(A numbered list any code improvements or efficiency tweaks)\n",
        "### Final Code ###\n",
        "(The final, revised version of the code.)\n",
        "\n",
        "For example, given the following input code to analyze in Python:\n",
        "```\n",
        "def bucketSort(arr, k):\n",
        "  counts = [0] * k\n",
        "  for x in arr:\n",
        "      counts[x] += 1\n",
        "\n",
        "  sorted_arr = []\n",
        "  for i, count in enumerate(arr):\n",
        "      sorted_arr.extend([i] * count)\n",
        "\n",
        "  return sorted_arr\n",
        "```\n",
        "\n",
        "Your response should be similar to the following output format (with more or fewer bugs and enhancements as needed):\n",
        "\n",
        "### Overview ###\n",
        "The provided `bucketSort` function has a significant logic error in how it reconstructs the sorted array and an inefficiency in its counting loop.\n",
        "### Bugs ###\n",
        "**Bug 1**: The second loop iterates over `arr` instead of `counts`. This will lead to an incorrect number of elements being added to `sorted_arr` and potentially an `IndexError` if elements in `arr` are greater than or equal to `k`.\n",
        "    **Fix**: Change `for i, count in enumerate(arr):` to `for i, count in enumerate(counts):` to iterate over the `counts` list.\n",
        "### Enhancements ###\n",
        "1. (Inefficiency): The first loop iterates through `arr` to populate `counts`. If `k` is much larger than the actual range of numbers in `arr`, this is fine. However, if the numbers in `arr` are densely packed and much smaller than `k`, it's still efficient. The primary inefficiency lies in the second loop's structure for reconstruction.\n",
        "2. (Clarity/Readability): The current approach is somewhat clear, but the bug makes it confusing. Once the bug is fixed, it's straightforward for its intended purpose.\n",
        "3. (Minor Efficiency Tweak): For the reconstruction part, `sorted_arr.extend([i] * count)` is generally efficient. An alternative for very large `count` values could be a list comprehension or generator expression, but `extend` is usually well-optimized in CPython. The primary improvement is fixing the logic.\n",
        "### Final Code ###\n",
        "```\n",
        "def bucketSort(arr, k):\n",
        "    counts = [0] * k\n",
        "    for x in arr:\n",
        "        # Ensure x is within the bounds of k. If not, this will raise an IndexError.\n",
        "        # A robust implementation might handle this by resizing k or raising a specific error.\n",
        "        counts[x] += 1\n",
        "\n",
        "    sorted_arr = []\n",
        "    # Iterate over the counts list to reconstruct the sorted array.\n",
        "    for i, count in enumerate(counts):\n",
        "        sorted_arr.extend([i] * count)\n",
        "\n",
        "    return sorted_arr\n",
        "```\n",
        "\n",
        "For the final revised code you write, return the code within triple backticks, and do not include any language identifier after the opening triple backticks.\n",
        "For example, notice how there is no language identifier after the opening triple backticks.\n",
        "This is an example of wrong output:\n",
        "```python\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "```\n",
        "This is an example of correct output:\n",
        "```\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "```\n",
        "Remember, no language identifier after the opening triple backticks after transcribing the code.\n",
        "\n",
        "Now, analyze the following code:\n",
        "```\n",
        "{code_block}\n",
        "```\n",
        "        \"\"\"\n",
        "        # Generate the response using the gen config\n",
        "        contents = [prompt]  # The LLM input is now a text prompt, not an image\n",
        "        response = model.generate_content(contents, generation_config=code_analysis_config)\n",
        "\n",
        "        # Get the response, and split\n",
        "        response_text = response.text\n",
        "        response_parts = response_text.split(\"### Final Code ###\")\n",
        "\n",
        "        # Grab the analysis and revised code\n",
        "        analysis = response_parts[0].strip()\n",
        "        revised_code = response_parts[1].strip()\n",
        "\n",
        "        # Clean up the code part\n",
        "        revised_code = revised_code.replace(\"```python\", \"\")\n",
        "        revised_code = revised_code.replace(\"```java\", \"\")\n",
        "        revised_code = revised_code.replace(\"```\", \"\")\n",
        "\n",
        "        return analysis, revised_code\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error during analysis: {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "example_image = Image.open(path_to_input_images + \"code_image_03.jpg\")\n",
        "code_language = \"Python\"\n",
        "image_text = transcribe_image_code(example_image, code_language)\n",
        "analysis, revised_code = analyze_code(image_text, code_language)\n",
        "\n",
        "print(\"##### Code transcription from image ######\\n\")\n",
        "print(image_text)\n",
        "\n",
        "print(\"\\n##### Code Analysis ######\\n\")\n",
        "print(analysis)\n",
        "\n",
        "print(\"\\n##### Revised Code ######\\n\")\n",
        "print(revised_code + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "01a112ea",
        "outputId": "222ce82e-c237-4a1c-d399-0c628aa86160"
      },
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "def gradio_pipeline(image, language) -> tuple[str, str, str]:\n",
        "  '''\n",
        "  gradio_pipeline transcribes the image of code, performs static analysis, and returns the analysis and revised code.\n",
        "\n",
        "  Parameters:\n",
        "      image (Image.Image): The uploaded image containing handwritten code.\n",
        "      language (str): The programming language of the code.\n",
        "\n",
        "  Returns:\n",
        "      (1) The transcribed code snippet.\n",
        "      (2) The analysis text explaining bugs, improvements, or efficiency suggestions.\n",
        "      (3) The refined version of the code with the suggested fixes and enhancements.\n",
        "  '''\n",
        "  transcribed = transcribe_image_code(image, language)\n",
        "  analysis, revised = analyze_code(transcribed, language)\n",
        "  return transcribed, analysis, revised\n",
        "\n",
        "def shutdown_app():\n",
        "  '''\n",
        "  shutdown_app shuts down the gradio app.\n",
        "  '''\n",
        "  print(\"Shutting down gradio app...\")\n",
        "  try:\n",
        "    app.close()\n",
        "  except Exception as e:\n",
        "    print(f\"Error during shutdown: {str(e)}\")\n",
        "\n",
        "with gr.Blocks(title = \"Whiteboard Tutor\") as app:\n",
        "  # Arrange UI elements. Create a single row with two columns\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      image_input = gr.Image(type=\"pil\", label=\"Upload Handwritten Code Image\")\n",
        "      lang = gr.Dropdown([\"Python\", \"Java\", \"Other\"], label=\"Code Language\", value=\"Python\")\n",
        "      run_button = gr.Button(\"Analyze Code\")\n",
        "    with gr.Column():\n",
        "      transcribe_output = gr.Code(label=\"Transcribed Code\", interactive=False)\n",
        "      analysis_output = gr.Markdown(label=\"Analysis\")\n",
        "      revised_output = gr.Code(label=\"Revised Code\", interactive=False)\n",
        "      close_button = gr.Button(\"Close App\")\n",
        "\n",
        "    run_button.click(gradio_pipeline, inputs=[image_input, lang], outputs=[transcribe_output, analysis_output, revised_output])\n",
        "    close_button.click(shutdown_app)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.launch(share=True, debug=True)"
      ],
      "id": "01a112ea",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b8ce601e51f91c8b84.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b8ce601e51f91c8b84.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1134, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 124, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 110, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 390, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 289, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7ad8259f6b40 [unset]> is bound to a different event loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w7ZN9gzReU_Z"
      },
      "id": "w7ZN9gzReU_Z"
    },
    {
      "cell_type": "code",
      "source": [
        "app.close()\n",
        "!pkill -f \"python.*gradio\"\n",
        "!pkill -f \"*gradio*\""
      ],
      "metadata": {
        "id": "U7tsYQqgoXTt"
      },
      "id": "U7tsYQqgoXTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f7430b3c",
      "metadata": {
        "id": "f7430b3c"
      },
      "source": [
        "### Additional Resource: Running test cases on LLM-generated code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5261dc1",
      "metadata": {
        "id": "f5261dc1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Example of running test cases on LLM-generated code.\n",
        "You do not need to follow this exact implementation for your code.\"\"\"\n",
        "\n",
        "import json\n",
        "import importlib.util\n",
        "\n",
        "def run_tests(filename_original, function_name, json_test_path):\n",
        "    # Load test cases\n",
        "    with open(json_test_path, 'r') as f:\n",
        "        test_cases = json.load(f)[\"test_case\"]\n",
        "\n",
        "    # Load the function from the file\n",
        "    spec = importlib.util.spec_from_file_location(function_name, filename_original)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "\n",
        "    #func = getattr(module, function_name)\n",
        "    func = getattr(module, [attr for attr in dir(module) if not attr.startswith('__')][0])\n",
        "\n",
        "    # Run tests\n",
        "    for idx, case in enumerate(test_cases):\n",
        "        try:\n",
        "            inputs = case[\"input\"]\n",
        "            expected = case[\"expected\"]\n",
        "            result = func(*inputs) if isinstance(inputs, (list, tuple)) else func(inputs)\n",
        "            assert result == expected, f\"input={inputs}, expected={expected}, got={result}\"\n",
        "            print(f\"Test {idx+1} passed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Test {idx+1} failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82f65c6",
      "metadata": {
        "id": "a82f65c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "079d476b-4330-49d3-c24a-4ed5fe955c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 passed.\n",
            "Test 2 passed.\n",
            "Test 3 passed.\n",
            "Test 4 passed.\n",
            "Test 5 passed.\n",
            "Test 6 passed.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Make sure to save the final code (after transcribing and performing static analysis) into a Python file.\n",
        "For example, if you have saved the final code transcribed and fixed by the LLM as example_llm_code.py,\n",
        "you can run the test cases using the format below.\n",
        "\n",
        "You can also add more inputs and expected outputs to the JSON file to run additional tests.\n",
        "It is encouraged to add more test cases to ensure the robustness of your code.\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "\n",
        "image_input_path = path_to_input_images + \"code_image_03.jpg\"\n",
        "image_output_path = path_to_output_images + \"revised_code_03.py\"\n",
        "path_to_json = GOOGLE_DRIVE_DIR_PREFIX + \"test_case_bucketsort.json\"\n",
        "\n",
        "input_image = Image.open(image_input_path)\n",
        "\n",
        "image_text = transcribe_image_code(input_image, \"Python\")\n",
        "analysis, revised_code = analyze_code(image_text, \"Python\")\n",
        "\n",
        "with Path(image_output_path).open(\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(revised_code)\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "run_tests(\n",
        "    filename_original=image_output_path,\n",
        "    function_name=\"bucketsort\",\n",
        "    json_test_path=path_to_json\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}